{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eba38a6-794e-40ce-91ee-459ebfdfe6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\result\\\\grnn_model_architecture.png'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchviz import make_dot\n",
    "\n",
    "Model_Scaler_Folder = \"../models/scaler/\"\n",
    "Model_PCA_Folder = \"../models/pca/\"\n",
    "\n",
    "Ticker_Hyperparams_Model_Metrics_Csv = \"hyperparameters_search_models/ticker_hyperparams_model_metrics.csv\"\n",
    "Ticker_Trained_Model_Metrics_Csv = \"trained_models/ticker_trained_model_metrics.csv\"\n",
    "\n",
    "Hyperparameters_Search_Models_Folder = \"hyperparameters_search_models/\"\n",
    "Trained_Models_Folder = \"trained_models/\"\n",
    "\n",
    "Hyperparameters_Search_Feature_Importance_Folder = \"hyperparameters_search_models/feature_importance/\"\n",
    "Trained_Models_Feature_Importance_Folder = \"trained_models/feature_importance/\"\n",
    "\n",
    "Trained_Feature_Folder ='../data/trained_feature/'\n",
    "PCA_Folder = '../data/pca/'\n",
    "\n",
    "ticker_symbol= \"Coffee\"\n",
    "Model_Type = \"grnn_regression\"\n",
    "\n",
    "X = pd.read_csv(f'{Trained_Feature_Folder}{ticker_symbol}.csv')\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_regressor = X[['DAILY_CLOSEPRICE_CHANGE']].copy(deep=True)  # Convert to DataFrame\n",
    "y = pd.DataFrame(y_scaler.fit_transform(y_regressor), columns=y_regressor.columns)\n",
    "\n",
    "X = X.drop('DAILY_CLOSEPRICE_CHANGE', axis=1)\n",
    "X = X.drop('Date', axis=1)\n",
    "\n",
    "Root_Folder = Model_Scaler_Folder\n",
    "\n",
    "gpu_available = True\n",
    "device = torch.device('cuda' if gpu_available and torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "trained_model_path = f'{Root_Folder}{Trained_Models_Folder}{Model_Type}/{ticker_symbol}_1.pth'\n",
    "trained_model_params_path = f'{Root_Folder}{Trained_Models_Folder}{Model_Type}/{ticker_symbol}_1.json'\n",
    "\n",
    "with open(trained_model_params_path, 'r') as f:\n",
    "    model_params = json.load(f)\n",
    "\n",
    "# Convert directly to tensors\n",
    "X = torch.tensor(X.values, dtype=torch.float32)\n",
    "input_size = X.shape[1]\n",
    "\n",
    "class GRNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, sigma, classification=False):\n",
    "        super(GRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.sigma = sigma\n",
    "        self.classification = classification\n",
    "\n",
    "        # Adding a linear layer to introduce trainable parameters\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "        # Apply He initialization to the linear layer\n",
    "        nn.init.kaiming_normal_(self.linear.weight, nonlinearity='relu')\n",
    "        if self.linear.bias is not None:\n",
    "            nn.init.constant_(self.linear.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        dist = torch.cdist(x, x)\n",
    "        weights = torch.exp(-dist ** 2 / (2 * self.sigma ** 2))\n",
    "        # Calculate the sum along the specified dimension\n",
    "        sums = weights.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # Check if sums are zero\n",
    "        zero_sums_mask = (sums == 0).squeeze()\n",
    "        if torch.any(zero_sums_mask):\n",
    "            weights[zero_sums_mask, :] = 0\n",
    "        else:\n",
    "            weights = weights / sums\n",
    "\n",
    "        output = torch.mm(weights, x)\n",
    "\n",
    "        if self.classification:\n",
    "            output = F.softmax(output, dim=1)\n",
    "        else:\n",
    "            output = self.linear(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "model = GRNN(input_size, 1, model_params['sigma'], classification=False).to(device)\n",
    "\n",
    "# Load the model state dict\n",
    "model.load_state_dict(torch.load(trained_model_path, map_location=device, weights_only=True), strict=False)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Visualize the model architecture using a sample from your data\n",
    "x_sample = X[0].unsqueeze(0).to(device)  # Use the first sample from your data\n",
    "y_sample = model(x_sample)\n",
    "dot = make_dot(y_sample, params=dict(model.named_parameters()))\n",
    "dot.format = 'png'\n",
    "dot.render('../result/grnn_model_architecture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "532f2970-088c-45d2-b825-4019ff7b7e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\result\\\\cnn_model_architecture.png'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchviz import make_dot\n",
    "\n",
    "Model_Scaler_Folder = \"../models/scaler/\"\n",
    "Model_PCA_Folder = \"../models/pca/\"\n",
    "\n",
    "Ticker_Hyperparams_Model_Metrics_Csv = \"hyperparameters_search_models/ticker_hyperparams_model_metrics.csv\"\n",
    "Ticker_Trained_Model_Metrics_Csv = \"trained_models/ticker_trained_model_metrics.csv\"\n",
    "\n",
    "Hyperparameters_Search_Models_Folder = \"hyperparameters_search_models/\"\n",
    "Trained_Models_Folder = \"trained_models/\"\n",
    "\n",
    "Hyperparameters_Search_Feature_Importance_Folder = \"hyperparameters_search_models/feature_importance/\"\n",
    "Trained_Models_Feature_Importance_Folder = \"trained_models/feature_importance/\"\n",
    "\n",
    "Trained_Feature_Folder ='../data/trained_feature/'\n",
    "PCA_Folder = '../data/pca/'\n",
    "\n",
    "ticker_symbol= \"Coffee\"\n",
    "Model_Type = \"conv1d_regression\"\n",
    "\n",
    "X = pd.read_csv(f'{Trained_Feature_Folder}{ticker_symbol}.csv')\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_regressor = X[['DAILY_CLOSEPRICE_CHANGE']].copy(deep=True)  # Convert to DataFrame\n",
    "y = pd.DataFrame(y_scaler.fit_transform(y_regressor), columns=y_regressor.columns)\n",
    "\n",
    "X = X.drop('DAILY_CLOSEPRICE_CHANGE', axis=1)\n",
    "X = X.drop('Date', axis=1)\n",
    "\n",
    "Root_Folder = Model_Scaler_Folder\n",
    "\n",
    "gpu_available = True\n",
    "device = torch.device('cuda' if gpu_available and torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "trained_model_path = f'{Root_Folder}{Trained_Models_Folder}{Model_Type}/{ticker_symbol}_1.pth'\n",
    "trained_model_params_path = f'{Root_Folder}{Trained_Models_Folder}{Model_Type}/{ticker_symbol}_1.json'\n",
    "\n",
    "with open(trained_model_params_path, 'r') as f:\n",
    "    model_params = json.load(f)\n",
    "\n",
    "\n",
    "class Conv1ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, l2_lambda=0.01, dropout_rate=0.5):\n",
    "        super(Conv1ResidualBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding='same')\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, stride=1, padding='same')\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.conv1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.conv2.weight, nonlinearity='relu')\n",
    "        nn.init.zeros_(self.conv1.bias)\n",
    "        nn.init.zeros_(self.conv2.bias)\n",
    "\n",
    "        self.l2_lambda = l2_lambda\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.residual_conv = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "        else:\n",
    "            self.residual_conv = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.residual_conv(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = out + residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Conv1DModel(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, l2_lambda=0.01, dropout_rate=0.5, num_blocks=1,\n",
    "                 classification=True):\n",
    "        super(Conv1DModel, self).__init__()\n",
    "        self.blocks = nn.Sequential(\n",
    "            Conv1ResidualBlock(in_channels, out_channels, kernel_size, l2_lambda=l2_lambda, dropout_rate=dropout_rate),\n",
    "            *[Conv1ResidualBlock(out_channels, out_channels, kernel_size, l2_lambda=l2_lambda,\n",
    "                                 dropout_rate=dropout_rate) for _ in range(num_blocks - 1)]\n",
    "        )\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)  # Global average pooling for 1D\n",
    "        self.fc = nn.Linear(out_channels, 2 if classification else 1)\n",
    "        self.classification = classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.blocks(x)\n",
    "        out = self.global_avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)  # Flatten the tensor\n",
    "        out = self.fc(out)\n",
    "        if self.classification:\n",
    "            out = F.log_softmax(out, dim=1)\n",
    "        return out\n",
    "\n",
    " # Convert directly to tensors\n",
    "X = torch.tensor(X.values, dtype=torch.float32).reshape((X.shape[0], 1, -1))\n",
    "in_channels = X.shape[1]\n",
    "out_channels = model_params['out_channels']\n",
    "kernel_size = model_params['kernel_size']\n",
    "num_blocks = model_params['num_blocks']\n",
    "l2_lambda = model_params['l2_lambda']\n",
    "dropout_rate = model_params['dropout_rate']\n",
    "batch_size = model_params['batch_size']\n",
    "\n",
    "# Initialize the model with the loaded parameters\n",
    "model = Conv1DModel(in_channels, out_channels, kernel_size, l2_lambda, dropout_rate, num_blocks, classification=False).to(device)\n",
    "\n",
    "# Load the model state dict\n",
    "model.load_state_dict(torch.load(trained_model_path, map_location=device, weights_only=True), strict=False)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Visualize the model architecture using a sample from your data\n",
    "x_sample = X[0].unsqueeze(0).to(device)  # Use the first sample from your data\n",
    "y_sample = model(x_sample)\n",
    "dot = make_dot(y_sample, params=dict(model.named_parameters()))\n",
    "dot.format = 'png'\n",
    "dot.render('../result/cnn_model_architecture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8937bcd-cabe-40ad-ac21-2a4c834752c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\result\\\\lstm_model_architecture.png'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchviz import make_dot\n",
    "\n",
    "Model_Scaler_Folder = \"../models/scaler/\"\n",
    "Model_PCA_Folder = \"../models/pca/\"\n",
    "\n",
    "Ticker_Hyperparams_Model_Metrics_Csv = \"hyperparameters_search_models/ticker_hyperparams_model_metrics.csv\"\n",
    "Ticker_Trained_Model_Metrics_Csv = \"trained_models/ticker_trained_model_metrics.csv\"\n",
    "\n",
    "Hyperparameters_Search_Models_Folder = \"hyperparameters_search_models/\"\n",
    "Trained_Models_Folder = \"trained_models/\"\n",
    "\n",
    "Hyperparameters_Search_Feature_Importance_Folder = \"hyperparameters_search_models/feature_importance/\"\n",
    "Trained_Models_Feature_Importance_Folder = \"trained_models/feature_importance/\"\n",
    "\n",
    "Trained_Feature_Folder ='../data/trained_feature/'\n",
    "PCA_Folder = '../data/pca/'\n",
    "\n",
    "ticker_symbol= \"Coffee\"\n",
    "Model_Type = \"lstm_regression\"\n",
    "\n",
    "X = pd.read_csv(f'{Trained_Feature_Folder}{ticker_symbol}.csv')\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_regressor = X[['DAILY_CLOSEPRICE_CHANGE']].copy(deep=True)  # Convert to DataFrame\n",
    "y = pd.DataFrame(y_scaler.fit_transform(y_regressor), columns=y_regressor.columns)\n",
    "\n",
    "X = X.drop('DAILY_CLOSEPRICE_CHANGE', axis=1)\n",
    "X = X.drop('Date', axis=1)\n",
    "\n",
    "Root_Folder = Model_Scaler_Folder\n",
    "\n",
    "gpu_available = True\n",
    "device = torch.device('cuda' if gpu_available and torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "trained_model_path = f'{Root_Folder}{Trained_Models_Folder}{Model_Type}/{ticker_symbol}_1.pth'\n",
    "trained_model_params_path = f'{Root_Folder}{Trained_Models_Folder}{Model_Type}/{ticker_symbol}_1.json'\n",
    "\n",
    "with open(trained_model_params_path, 'r') as f:\n",
    "    model_params = json.load(f)\n",
    "\n",
    "\n",
    "class LSTMResidualBlock(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, l2_lambda=0.01, dropout_rate=0.5, num_layers=1):\n",
    "        super(LSTMResidualBlock, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True,\n",
    "                            dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        if input_size != hidden_size:\n",
    "            self.residual_fc = nn.Linear(input_size, hidden_size)\n",
    "        else:\n",
    "            self.residual_fc = nn.Identity()\n",
    "\n",
    "        self.l2_lambda = l2_lambda\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.residual_fc(x)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.dropout(out)\n",
    "        out = self.layer_norm(out + residual)\n",
    "        return out\n",
    "\n",
    "    def l2_regularization_loss(self):\n",
    "        l2_loss = 0\n",
    "        for param in self.parameters():\n",
    "            l2_loss += torch.norm(param, 2)\n",
    "        return self.l2_lambda * l2_loss\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, l2_lambda=0.01, dropout_rate=0.5, num_layers=1, num_blocks=1, classification=True):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.blocks = nn.Sequential(\n",
    "            LSTMResidualBlock(input_size, hidden_size, l2_lambda=l2_lambda, dropout_rate=dropout_rate, num_layers=num_layers),\n",
    "            *[LSTMResidualBlock(hidden_size, hidden_size, l2_lambda=l2_lambda, dropout_rate=dropout_rate, num_layers=num_layers) for _ in\n",
    "              range(num_blocks - 1)]\n",
    "        )\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)  # Global average pooling for 1D\n",
    "        self.fc = nn.Linear(hidden_size, 2 if classification else 1)\n",
    "        self.classification = classification\n",
    "        self.l2_lambda = l2_lambda\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.blocks(x)\n",
    "        out = out.mean(dim=1)  # Global average pooling\n",
    "        out = self.fc(out)\n",
    "        if self.classification:\n",
    "            out = F.log_softmax(out, dim=1)\n",
    "        return out\n",
    "\n",
    "    def l2_regularization_loss(self):\n",
    "        l2_loss = 0\n",
    "        for param in self.parameters():\n",
    "            l2_loss += torch.norm(param, 2)\n",
    "        return self.l2_lambda * l2_loss\n",
    "\n",
    "def create_lstm_train_sequences(X, y, sequence_length):\n",
    "    sequences_X, sequences_y = [], []\n",
    "    for i in range(len(X) - sequence_length + 1):\n",
    "        sequences_X.append(X[i:i + sequence_length])\n",
    "        sequences_y.append(y[i + sequence_length - 1])\n",
    "    return torch.stack(sequences_X), torch.stack(sequences_y)\n",
    "\n",
    "def create_lstm_predict_sequences(X, sequence_length):\n",
    "    sequences = []\n",
    "    for i in range(len(X) - sequence_length + 1):\n",
    "        seq = X[i:i + sequence_length]\n",
    "        sequences.append(seq)\n",
    "    return torch.stack(sequences)\n",
    "\n",
    "\n",
    "X = torch.tensor(X.values, dtype=torch.float32)\n",
    "sequence_length = model_params['sequence_length']\n",
    "hidden_size = model_params['hidden_size']\n",
    "num_layers = model_params['num_layers']\n",
    "num_blocks = model_params['num_blocks']\n",
    "l2_lambda = model_params['l2_lambda']\n",
    "dropout_rate = model_params['dropout_rate']\n",
    "batch_size = model_params['batch_size']\n",
    "\n",
    "# Create sequences from X\n",
    "X_seq = create_lstm_predict_sequences(X, sequence_length)\n",
    "\n",
    "input_size = X_seq.shape[2]\n",
    "\n",
    "# Initialize the model with the loaded parameters and move it to the device\n",
    "model = LSTMModel(input_size, hidden_size, l2_lambda, dropout_rate, num_layers, num_blocks, classification=False).to(device)\n",
    "model.load_state_dict(torch.load(trained_model_path, map_location=device, weights_only=True), strict=False)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Visualize the model architecture using a sample from your data\n",
    "x_sample = X_seq[0].unsqueeze(0).to(device)  # Use the first sample from your data\n",
    "y_sample = model(x_sample)\n",
    "dot = make_dot(y_sample, params=dict(model.named_parameters()))\n",
    "dot.format = 'png'\n",
    "dot.render('../result/lstm_model_architecture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a05af557-39f3-4bac-afb5-5c91f0d3e5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\result\\\\cnn_lstm_model_architecture.png'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchviz import make_dot\n",
    "\n",
    "Model_Scaler_Folder = \"../models/scaler/\"\n",
    "Model_PCA_Folder = \"../models/pca/\"\n",
    "\n",
    "Ticker_Hyperparams_Model_Metrics_Csv = \"hyperparameters_search_models/ticker_hyperparams_model_metrics.csv\"\n",
    "Ticker_Trained_Model_Metrics_Csv = \"trained_models/ticker_trained_model_metrics.csv\"\n",
    "\n",
    "Hyperparameters_Search_Models_Folder = \"hyperparameters_search_models/\"\n",
    "Trained_Models_Folder = \"trained_models/\"\n",
    "\n",
    "Hyperparameters_Search_Feature_Importance_Folder = \"hyperparameters_search_models/feature_importance/\"\n",
    "Trained_Models_Feature_Importance_Folder = \"trained_models/feature_importance/\"\n",
    "\n",
    "Trained_Feature_Folder ='../data/trained_feature/'\n",
    "PCA_Folder = '../data/pca/'\n",
    "\n",
    "ticker_symbol= \"Coffee\"\n",
    "Model_Type = \"conv1d_lstm_regression\"\n",
    "\n",
    "X = pd.read_csv(f'{Trained_Feature_Folder}{ticker_symbol}.csv')\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_regressor = X[['DAILY_CLOSEPRICE_CHANGE']].copy(deep=True)  # Convert to DataFrame\n",
    "y = pd.DataFrame(y_scaler.fit_transform(y_regressor), columns=y_regressor.columns)\n",
    "\n",
    "X = X.drop('DAILY_CLOSEPRICE_CHANGE', axis=1)\n",
    "X = X.drop('Date', axis=1)\n",
    "\n",
    "Root_Folder = Model_Scaler_Folder\n",
    "\n",
    "gpu_available = True\n",
    "device = torch.device('cuda' if gpu_available and torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "trained_model_path = f'{Root_Folder}{Trained_Models_Folder}{Model_Type}/{ticker_symbol}_1.pth'\n",
    "trained_model_params_path = f'{Root_Folder}{Trained_Models_Folder}{Model_Type}/{ticker_symbol}_1.json'\n",
    "\n",
    "with open(trained_model_params_path, 'r') as f:\n",
    "    model_params = json.load(f)\n",
    "\n",
    "\n",
    "class Conv1ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, l2_lambda=0.01, dropout_rate=0.5):\n",
    "        super(Conv1ResidualBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding='same')\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, stride=1, padding='same')\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.conv1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.conv2.weight, nonlinearity='relu')\n",
    "        nn.init.zeros_(self.conv1.bias)\n",
    "        nn.init.zeros_(self.conv2.bias)\n",
    "\n",
    "        self.l2_lambda = l2_lambda\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.residual_conv = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "        else:\n",
    "            self.residual_conv = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.residual_conv(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = out + residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Conv1DLSTMModel(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, num_blocks=1, lstm_hidden_size=64, l2_lambda=0.01, dropout_rate=0.5,\n",
    "                 classification=True):\n",
    "        super(Conv1DLSTMModel, self).__init__()\n",
    "        self.blocks = nn.Sequential(\n",
    "            Conv1ResidualBlock(in_channels, out_channels, kernel_size, l2_lambda=l2_lambda, dropout_rate=dropout_rate),\n",
    "            *[Conv1ResidualBlock(out_channels, out_channels, kernel_size, l2_lambda=l2_lambda,\n",
    "                                 dropout_rate=dropout_rate) for _ in range(num_blocks - 1)]\n",
    "        )\n",
    "        self.lstm = nn.LSTM(out_channels, lstm_hidden_size, batch_first=True)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)  # Global average pooling for 1D\n",
    "        self.fc = nn.Linear(lstm_hidden_size, 2 if classification else 1)\n",
    "        self.classification = classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.blocks(x)\n",
    "        out = out.permute(0, 2, 1)  # Change shape to (batch_size, sequence_length, input_size) position\n",
    "        out, _ = self.lstm(out)\n",
    "        out = self.global_avg_pool(\n",
    "            out.permute(0, 2, 1))  # Change shape back to (batch_size, input_size, sequence_length) position\n",
    "        out = out.view(out.size(0), -1)  # Flatten the tensor\n",
    "        out = self.fc(out)\n",
    "        if self.classification:\n",
    "            out = F.log_softmax(out, dim=1)\n",
    "        return out\n",
    "\n",
    "# Convert directly to tensors\n",
    "X = torch.tensor(X.values, dtype=torch.float32).reshape((X.shape[0], 1, -1))\n",
    "in_channels = X.shape[1]\n",
    "out_channels = model_params['out_channels']\n",
    "kernel_size = model_params['kernel_size']\n",
    "num_blocks = model_params['num_blocks']\n",
    "lstm_hidden_size = model_params['lstm_hidden_size']\n",
    "l2_lambda = model_params['l2_lambda']\n",
    "dropout_rate = model_params['dropout_rate']\n",
    "batch_size = model_params['batch_size']\n",
    "\n",
    "# Initialize the model with the loaded parameters\n",
    "model = Conv1DLSTMModel(in_channels, out_channels, kernel_size, num_blocks, lstm_hidden_size, l2_lambda, dropout_rate, classification=False).to(device)\n",
    "model.load_state_dict(torch.load(trained_model_path, map_location=device, weights_only=True), strict=False)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Visualize the model architecture using a sample from your data\n",
    "x_sample = X[0].unsqueeze(0).to(device)  # Use the first sample from your data\n",
    "y_sample = model(x_sample)\n",
    "dot = make_dot(y_sample, params=dict(model.named_parameters()))\n",
    "dot.format = 'png'\n",
    "dot.render('../result/cnn_lstm_model_architecture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc44b7bb-1681-45d6-a1c8-2a2cc0a95cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
