digraph {
	graph [size="25.65,25.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2476068175728 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	2478002321392 [label=AddmmBackward0]
	2478002321488 -> 2478002321392
	2476068168768 [label="fc.bias
 (1)" fillcolor=lightblue]
	2476068168768 -> 2478002321488
	2478002321488 [label=AccumulateGrad]
	2478002307712 -> 2478002321392
	2478002307712 [label=ViewBackward0]
	2478002322112 -> 2478002307712
	2478002322112 [label=SqueezeBackward1]
	2478002519488 -> 2478002322112
	2478002519488 [label=MeanBackward1]
	2478002507200 -> 2478002519488
	2478002507200 [label=UnsqueezeBackward0]
	2478002507728 -> 2478002507200
	2478002507728 [label=ReluBackward0]
	2477297572960 -> 2478002507728
	2477297572960 [label=AddBackward0]
	2478002507776 -> 2477297572960
	2478002507776 [label=CudnnBatchNormBackward0]
	2478002510848 -> 2478002507776
	2478002510848 [label=ConvolutionBackward0]
	2478002508880 -> 2478002510848
	2478002508880 [label=ReluBackward0]
	2478002506480 -> 2478002508880
	2478002506480 [label=CudnnBatchNormBackward0]
	2478002507008 -> 2478002506480
	2478002507008 [label=ConvolutionBackward0]
	2478002508736 -> 2478002507008
	2478002508736 [label=ReluBackward0]
	2478002518240 -> 2478002508736
	2478002518240 [label=AddBackward0]
	2478002517904 -> 2478002518240
	2478002517904 [label=CudnnBatchNormBackward0]
	2478002517472 -> 2478002517904
	2478002517472 [label=ConvolutionBackward0]
	2478002516656 -> 2478002517472
	2478002516656 [label=ReluBackward0]
	2478002516128 -> 2478002516656
	2478002516128 [label=CudnnBatchNormBackward0]
	2478002515264 -> 2478002516128
	2478002515264 [label=ConvolutionBackward0]
	2478002517952 -> 2478002515264
	2478002517952 [label=ReluBackward0]
	2478002513296 -> 2478002517952
	2478002513296 [label=AddBackward0]
	2478002514448 -> 2478002513296
	2478002514448 [label=CudnnBatchNormBackward0]
	2478002515312 -> 2478002514448
	2478002515312 [label=ConvolutionBackward0]
	2478002508352 -> 2478002515312
	2478002508352 [label=ReluBackward0]
	2478002509600 -> 2478002508352
	2478002509600 [label=CudnnBatchNormBackward0]
	2478002518960 -> 2478002509600
	2478002518960 [label=ConvolutionBackward0]
	2478002519056 -> 2478002518960
	2476067906224 [label="blocks.0.conv1.weight
 (79, 1, 5)" fillcolor=lightblue]
	2476067906224 -> 2478002519056
	2478002519056 [label=AccumulateGrad]
	2478002508544 -> 2478002518960
	2476067907024 [label="blocks.0.conv1.bias
 (79)" fillcolor=lightblue]
	2476067907024 -> 2478002508544
	2478002508544 [label=AccumulateGrad]
	2478002508592 -> 2478002509600
	2476067905264 [label="blocks.0.bn1.weight
 (79)" fillcolor=lightblue]
	2476067905264 -> 2478002508592
	2478002508592 [label=AccumulateGrad]
	2478002508928 -> 2478002509600
	2476067905344 [label="blocks.0.bn1.bias
 (79)" fillcolor=lightblue]
	2476067905344 -> 2478002508928
	2478002508928 [label=AccumulateGrad]
	2478002510560 -> 2478002515312
	2476067904144 [label="blocks.0.conv2.weight
 (79, 79, 5)" fillcolor=lightblue]
	2476067904144 -> 2478002510560
	2478002510560 [label=AccumulateGrad]
	2478002519680 -> 2478002515312
	2476067904224 [label="blocks.0.conv2.bias
 (79)" fillcolor=lightblue]
	2476067904224 -> 2478002519680
	2478002519680 [label=AccumulateGrad]
	2478002519872 -> 2478002514448
	2476067903424 [label="blocks.0.bn2.weight
 (79)" fillcolor=lightblue]
	2476067903424 -> 2478002519872
	2478002519872 [label=AccumulateGrad]
	2478002514688 -> 2478002514448
	2476068165488 [label="blocks.0.bn2.bias
 (79)" fillcolor=lightblue]
	2476068165488 -> 2478002514688
	2478002514688 [label=AccumulateGrad]
	2478002514880 -> 2478002513296
	2478002514880 [label=ConvolutionBackward0]
	2478002510416 -> 2478002514880
	2476068165408 [label="blocks.0.residual_conv.weight
 (79, 1, 1)" fillcolor=lightblue]
	2476068165408 -> 2478002510416
	2478002510416 [label=AccumulateGrad]
	2478002511760 -> 2478002514880
	2476068167968 [label="blocks.0.residual_conv.bias
 (79)" fillcolor=lightblue]
	2476068167968 -> 2478002511760
	2478002511760 [label=AccumulateGrad]
	2478002515120 -> 2478002515264
	2476068164528 [label="blocks.1.conv1.weight
 (79, 79, 5)" fillcolor=lightblue]
	2476068164528 -> 2478002515120
	2478002515120 [label=AccumulateGrad]
	2478002515408 -> 2478002515264
	2476068165088 [label="blocks.1.conv1.bias
 (79)" fillcolor=lightblue]
	2476068165088 -> 2478002515408
	2478002515408 [label=AccumulateGrad]
	2478002515744 -> 2478002516128
	2476068165968 [label="blocks.1.bn1.weight
 (79)" fillcolor=lightblue]
	2476068165968 -> 2478002515744
	2478002515744 [label=AccumulateGrad]
	2478002516464 -> 2478002516128
	2476068166048 [label="blocks.1.bn1.bias
 (79)" fillcolor=lightblue]
	2476068166048 -> 2478002516464
	2478002516464 [label=AccumulateGrad]
	2478002516992 -> 2478002517472
	2476068166448 [label="blocks.1.conv2.weight
 (79, 79, 5)" fillcolor=lightblue]
	2476068166448 -> 2478002516992
	2478002516992 [label=AccumulateGrad]
	2478002516800 -> 2478002517472
	2476068166528 [label="blocks.1.conv2.bias
 (79)" fillcolor=lightblue]
	2476068166528 -> 2478002516800
	2478002516800 [label=AccumulateGrad]
	2478002517616 -> 2478002517904
	2476068166608 [label="blocks.1.bn2.weight
 (79)" fillcolor=lightblue]
	2476068166608 -> 2478002517616
	2478002517616 [label=AccumulateGrad]
	2478002517568 -> 2478002517904
	2476068166688 [label="blocks.1.bn2.bias
 (79)" fillcolor=lightblue]
	2476068166688 -> 2478002517568
	2478002517568 [label=AccumulateGrad]
	2478002517952 -> 2478002518240
	2478002507392 -> 2478002507008
	2476068167168 [label="blocks.2.conv1.weight
 (79, 79, 5)" fillcolor=lightblue]
	2476068167168 -> 2478002507392
	2478002507392 [label=AccumulateGrad]
	2478002507440 -> 2478002507008
	2476068167248 [label="blocks.2.conv1.bias
 (79)" fillcolor=lightblue]
	2476068167248 -> 2478002507440
	2478002507440 [label=AccumulateGrad]
	2478002506960 -> 2478002506480
	2476068167328 [label="blocks.2.bn1.weight
 (79)" fillcolor=lightblue]
	2476068167328 -> 2478002506960
	2478002506960 [label=AccumulateGrad]
	2478002506672 -> 2478002506480
	2476068167408 [label="blocks.2.bn1.bias
 (79)" fillcolor=lightblue]
	2476068167408 -> 2478002506672
	2478002506672 [label=AccumulateGrad]
	2478002510512 -> 2478002510848
	2476068167808 [label="blocks.2.conv2.weight
 (79, 79, 5)" fillcolor=lightblue]
	2476068167808 -> 2478002510512
	2478002510512 [label=AccumulateGrad]
	2478002510128 -> 2478002510848
	2476068165008 [label="blocks.2.conv2.bias
 (79)" fillcolor=lightblue]
	2476068165008 -> 2478002510128
	2478002510128 [label=AccumulateGrad]
	2478002516608 -> 2478002507776
	2476068167888 [label="blocks.2.bn2.weight
 (79)" fillcolor=lightblue]
	2476068167888 -> 2478002516608
	2478002516608 [label=AccumulateGrad]
	2478002514592 -> 2478002507776
	2476068168048 [label="blocks.2.bn2.bias
 (79)" fillcolor=lightblue]
	2476068168048 -> 2478002514592
	2478002514592 [label=AccumulateGrad]
	2478002508736 -> 2477297572960
	2478002307376 -> 2478002321392
	2478002307376 [label=TBackward0]
	2478002513152 -> 2478002307376
	2476068168688 [label="fc.weight
 (1, 79)" fillcolor=lightblue]
	2476068168688 -> 2478002513152
	2478002513152 [label=AccumulateGrad]
	2478002321392 -> 2476068175728
}
