{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eabd5554-f780-4ca2-b0d7-fbfb52f47087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from urllib.parse import quote\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "import os\n",
    "import requests\n",
    "import gdelt\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "022bcc96-acc3-4fa7-8e2c-086bc8dd12a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ../nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Define the download directory\n",
    "download_dir = os.path.join('../nltk')\n",
    "\n",
    "# Download the 'punkt' package to the specified directory\n",
    "nltk.download('punkt', download_dir=download_dir)\n",
    "\n",
    "# Add the download directory to the NLTK data path\n",
    "nltk.data.path.append(download_dir)\n",
    "# Load the Loughran-McDonald word lists\n",
    "lm_dict = pd.read_csv('../nltk/Loughran-McDonald_MasterDictionary_1993-2023.csv')\n",
    "\n",
    "# Extract sentiment word lists\n",
    "negative_words = lm_dict[lm_dict['Negative'] > 0]['Word'].tolist()\n",
    "negative_words = [w.lower() for w in negative_words]\n",
    "positive_words = lm_dict[lm_dict['Positive'] > 0]['Word'].tolist()\n",
    "postiive_words = [w.lower() for w in positive_words]\n",
    "uncertainty_words = lm_dict[lm_dict['Uncertainty'] > 0]['Word'].tolist()\n",
    "uncertainty_words = [w.lower() for w in uncertainty_words]\n",
    "litigious_words = lm_dict[lm_dict['Litigious'] > 0]['Word'].tolist()\n",
    "litigious_words = [w.lower() for w in litigious_words]\n",
    "strong_modal_words = lm_dict[lm_dict['Strong_Modal'] > 0]['Word'].tolist()\n",
    "strong_modal_words = [w.lower() for w in strong_modal_words]\n",
    "weak_modal_words = lm_dict[lm_dict['Weak_Modal'] > 0]['Word'].tolist()\n",
    "weak_modal_words = [w.lower() for w in weak_modal_words]\n",
    "constraining_words = lm_dict[lm_dict['Constraining'] > 0]['Word'].tolist()\n",
    "constraining_words = [w.lower() for w in constraining_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "902cfc9d-ec6a-4724-aa84-e1335e78af73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_sentiment_score(df):\n",
    "    # Create an empty sentiment dataframe with the necessary columns\n",
    "    sentiment_df = pd.DataFrame({\n",
    "        'Date': df['published_date'],\n",
    "        'SENTIMENT_POSITIVE': 0.0,\n",
    "        'SENTIMENT_NEGATIVE': 0.0,\n",
    "        'SENTIMENT_UNCERTAINTY': 0.0,\n",
    "        'SENTIMENT_LITIGIOUS': 0.0,\n",
    "        'SENTIMENT_STRONG_MODAL': 0.0,\n",
    "        'SENTIMENT_WEAK_MODAL': 0.0,\n",
    "        'SENTIMENT_CONSTRAINING': 0.0\n",
    "    })\n",
    "\n",
    "    # Check for any NaT (invalid date) values\n",
    "    print(sentiment_df[sentiment_df['Date'].isna()])\n",
    "\n",
    "    # Iterate over each title and analyze the sentiment\n",
    "    for i, row in df.iterrows():\n",
    "        words = row['title'].lower().split()  # Split the title into words\n",
    "        for word in words:\n",
    "            if word in negative_words:\n",
    "                sentiment_df.loc[i, 'SENTIMENT_NEGATIVE'] += 1.0\n",
    "            if word in positive_words:\n",
    "                sentiment_df.loc[i, 'SENTIMENT_POSITIVE'] += 1.0\n",
    "            if word in uncertainty_words:\n",
    "                sentiment_df.loc[i, 'SENTIMENT_UNCERTAINTY'] += 1.0\n",
    "            if word in litigious_words:\n",
    "                sentiment_df.loc[i, 'SENTIMENT_LITIGIOUS'] += 1.0\n",
    "            if word in strong_modal_words:\n",
    "                sentiment_df.loc[i, 'SENTIMENT_STRONG_MODAL'] += 1.0\n",
    "            if word in weak_modal_words:\n",
    "                sentiment_df.loc[i, 'SENTIMENT_WEAK_MODAL'] += 1.0\n",
    "            if word in constraining_words:\n",
    "                sentiment_df.loc[i, 'SENTIMENT_CONSTRAINING'] += 1.0\n",
    "\n",
    "    # Add the composite sentiment features\n",
    "    sentiment_df[\"NET_SENTIMENT\"] = sentiment_df[\"SENTIMENT_POSITIVE\"] - sentiment_df[\"SENTIMENT_NEGATIVE\"]\n",
    "    sentiment_df[\"UNCERTAINTY_MODAL\"] = sentiment_df[\"SENTIMENT_UNCERTAINTY\"] + sentiment_df[\"SENTIMENT_STRONG_MODAL\"] + sentiment_df[\"SENTIMENT_WEAK_MODAL\"]\n",
    "    sentiment_df[\"REGULATORY_PRESSURE\"] = sentiment_df[\"SENTIMENT_LITIGIOUS\"] + sentiment_df[\"SENTIMENT_CONSTRAINING\"]\n",
    "\n",
    "    sentiment_df = sentiment_df.drop([\n",
    "        'SENTIMENT_POSITIVE',\n",
    "        'SENTIMENT_NEGATIVE',\n",
    "        'SENTIMENT_UNCERTAINTY',\n",
    "        'SENTIMENT_LITIGIOUS',\n",
    "        'SENTIMENT_STRONG_MODAL',\n",
    "        'SENTIMENT_WEAK_MODAL',\n",
    "        'SENTIMENT_CONSTRAINING'], axis=1)\n",
    "\n",
    "    # Group by 'Date' and sum the sentiment scores for rows with the same Date\n",
    "    aggregated_sentiment_df = sentiment_df.groupby('Date').sum().reset_index()\n",
    "\n",
    "    # Calculate and print the percentage of zero values in each column\n",
    "    zero_percentage = (aggregated_sentiment_df == 0).mean() * 100\n",
    "    print(\"\\nPercentage of zeros in each column:\")\n",
    "    print(zero_percentage)\n",
    "\n",
    "    # Print the max values for each column\n",
    "    max_values = aggregated_sentiment_df.max()\n",
    "    print(\"\\nMax values in each column:\")\n",
    "    print(max_values)\n",
    "\n",
    "    # Print the min values for each column\n",
    "    min_values = aggregated_sentiment_df.min()\n",
    "    print(\"\\nMin values in each column:\")\n",
    "    print(min_values)\n",
    "\n",
    "    # List of columns to apply scaling to\n",
    "    columns_to_scale = ['NET_SENTIMENT', 'UNCERTAINTY_MODAL', 'REGULATORY_PRESSURE']\n",
    "\n",
    "    # Apply the scaling function\n",
    "    scaled_sentiment_df = scale_sentiments(aggregated_sentiment_df, columns_to_scale)\n",
    "\n",
    "    return scaled_sentiment_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "935695a2-7674-447c-873a-95dc0fdd0fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_commod_data_from_csv(file_path):\n",
    "  df = pd.read_csv(file_path)\n",
    "  df.rename(columns={'Settle': 'Close'}, inplace=True)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7bc2288-6987-4aab-b0f4-1c410b66fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_negative(value, min_negative):\n",
    "    return 1 + ((value - min_negative) / (0 - min_negative)) * (4 - 1)  # Scaling between 1 and 4\n",
    "\n",
    "def scale_positive(value, max_positive):\n",
    "    return 6 + (value / max_positive) * (10 - 6)  # Scaling between 6 and 10\n",
    "\n",
    "# Apply scaling\n",
    "def custom_scale(value, min_negative, max_positive):\n",
    "    if value < 0:\n",
    "        return scale_negative(value, min_negative)\n",
    "    elif value > 0:\n",
    "        return scale_positive(value, max_positive)\n",
    "    else:\n",
    "        return 5  # Zero mapped to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27ee421b-ccd7-4705-9b65-e273e671b5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scale sentiment columns\n",
    "def scale_sentiments(sentiment_df, columns):\n",
    "    for col in columns:\n",
    "        # Get min and max values for negative and positive sentiments for each column\n",
    "        min_negative = sentiment_df[sentiment_df[col] < 0][col].min()\n",
    "        max_positive = sentiment_df[sentiment_df[col] > 0][col].max()\n",
    "        \n",
    "        # Apply the custom scaling for each column\n",
    "        sentiment_df[f'scaled_{col}'] = sentiment_df[col].apply(\n",
    "            lambda x: custom_scale(x, min_negative, max_positive)\n",
    "        )\n",
    "    return sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9856541d-05d2-42b0-98f6-026143e706fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Date, SENTIMENT_POSITIVE, SENTIMENT_NEGATIVE, SENTIMENT_UNCERTAINTY, SENTIMENT_LITIGIOUS, SENTIMENT_STRONG_MODAL, SENTIMENT_WEAK_MODAL, SENTIMENT_CONSTRAINING]\n",
      "Index: []\n",
      "\n",
      "Percentage of zeros in each column:\n",
      "Date                    0.000000\n",
      "NET_SENTIMENT          87.224670\n",
      "UNCERTAINTY_MODAL      85.903084\n",
      "REGULATORY_PRESSURE    96.475771\n",
      "dtype: float64\n",
      "\n",
      "Max values in each column:\n",
      "Date                   2021-06-27\n",
      "NET_SENTIMENT                 0.0\n",
      "UNCERTAINTY_MODAL             4.0\n",
      "REGULATORY_PRESSURE           2.0\n",
      "dtype: object\n",
      "\n",
      "Min values in each column:\n",
      "Date                   2014-01-03\n",
      "NET_SENTIMENT                -4.0\n",
      "UNCERTAINTY_MODAL             0.0\n",
      "REGULATORY_PRESSURE           0.0\n",
      "dtype: object\n",
      "Sentiment scores saved for Cocoa at ../data/news_sentiments_scores/Cocoa_sentiment.csv\n",
      "Empty DataFrame\n",
      "Columns: [Date, SENTIMENT_POSITIVE, SENTIMENT_NEGATIVE, SENTIMENT_UNCERTAINTY, SENTIMENT_LITIGIOUS, SENTIMENT_STRONG_MODAL, SENTIMENT_WEAK_MODAL, SENTIMENT_CONSTRAINING]\n",
      "Index: []\n",
      "\n",
      "Percentage of zeros in each column:\n",
      "Date                    0.000000\n",
      "NET_SENTIMENT          39.442815\n",
      "UNCERTAINTY_MODAL      52.126100\n",
      "REGULATORY_PRESSURE    84.897361\n",
      "dtype: float64\n",
      "\n",
      "Max values in each column:\n",
      "Date                   2021-06-29 00:00:00\n",
      "NET_SENTIMENT                          0.0\n",
      "UNCERTAINTY_MODAL                      9.0\n",
      "REGULATORY_PRESSURE                    3.0\n",
      "dtype: object\n",
      "\n",
      "Min values in each column:\n",
      "Date                   2014-01-02 00:00:00\n",
      "NET_SENTIMENT                         -8.0\n",
      "UNCERTAINTY_MODAL                      0.0\n",
      "REGULATORY_PRESSURE                    0.0\n",
      "dtype: object\n",
      "Sentiment scores saved for Coffee at ../data/news_sentiments_scores/Coffee_sentiment.csv\n",
      "Empty DataFrame\n",
      "Columns: [Date, SENTIMENT_POSITIVE, SENTIMENT_NEGATIVE, SENTIMENT_UNCERTAINTY, SENTIMENT_LITIGIOUS, SENTIMENT_STRONG_MODAL, SENTIMENT_WEAK_MODAL, SENTIMENT_CONSTRAINING]\n",
      "Index: []\n",
      "\n",
      "Percentage of zeros in each column:\n",
      "Date                    0.000000\n",
      "NET_SENTIMENT          77.378702\n",
      "UNCERTAINTY_MODAL      82.356648\n",
      "REGULATORY_PRESSURE    95.148078\n",
      "dtype: float64\n",
      "\n",
      "Max values in each column:\n",
      "Date                   2021-06-24\n",
      "NET_SENTIMENT                 0.0\n",
      "UNCERTAINTY_MODAL             5.0\n",
      "REGULATORY_PRESSURE           2.0\n",
      "dtype: object\n",
      "\n",
      "Min values in each column:\n",
      "Date                   2014-01-04\n",
      "NET_SENTIMENT                -5.0\n",
      "UNCERTAINTY_MODAL             0.0\n",
      "REGULATORY_PRESSURE           0.0\n",
      "dtype: object\n",
      "Sentiment scores saved for Corn at ../data/news_sentiments_scores/Corn_sentiment.csv\n",
      "Empty DataFrame\n",
      "Columns: [Date, SENTIMENT_POSITIVE, SENTIMENT_NEGATIVE, SENTIMENT_UNCERTAINTY, SENTIMENT_LITIGIOUS, SENTIMENT_STRONG_MODAL, SENTIMENT_WEAK_MODAL, SENTIMENT_CONSTRAINING]\n",
      "Index: []\n",
      "\n",
      "Percentage of zeros in each column:\n",
      "Date                    0.000000\n",
      "NET_SENTIMENT          68.552570\n",
      "UNCERTAINTY_MODAL      80.810938\n",
      "REGULATORY_PRESSURE    92.079208\n",
      "dtype: float64\n",
      "\n",
      "Max values in each column:\n",
      "Date                   2021-06-28 00:00:00\n",
      "NET_SENTIMENT                          0.0\n",
      "UNCERTAINTY_MODAL                      5.0\n",
      "REGULATORY_PRESSURE                    3.0\n",
      "dtype: object\n",
      "\n",
      "Min values in each column:\n",
      "Date                   2014-01-02 00:00:00\n",
      "NET_SENTIMENT                         -9.0\n",
      "UNCERTAINTY_MODAL                      0.0\n",
      "REGULATORY_PRESSURE                    0.0\n",
      "dtype: object\n",
      "Sentiment scores saved for Cotton at ../data/news_sentiments_scores/Cotton_sentiment.csv\n",
      "Empty DataFrame\n",
      "Columns: [Date, SENTIMENT_POSITIVE, SENTIMENT_NEGATIVE, SENTIMENT_UNCERTAINTY, SENTIMENT_LITIGIOUS, SENTIMENT_STRONG_MODAL, SENTIMENT_WEAK_MODAL, SENTIMENT_CONSTRAINING]\n",
      "Index: []\n",
      "\n",
      "Percentage of zeros in each column:\n",
      "Date                    0.000000\n",
      "NET_SENTIMENT          78.443114\n",
      "UNCERTAINTY_MODAL      79.586282\n",
      "REGULATORY_PRESSURE    95.971693\n",
      "dtype: float64\n",
      "\n",
      "Max values in each column:\n",
      "Date                   2021-06-27\n",
      "NET_SENTIMENT                 0.0\n",
      "UNCERTAINTY_MODAL             4.0\n",
      "REGULATORY_PRESSURE           3.0\n",
      "dtype: object\n",
      "\n",
      "Min values in each column:\n",
      "Date                   2014-01-04\n",
      "NET_SENTIMENT                -4.0\n",
      "UNCERTAINTY_MODAL             0.0\n",
      "REGULATORY_PRESSURE           0.0\n",
      "dtype: object\n",
      "Sentiment scores saved for Soya Bean at ../data/news_sentiments_scores/Soya Bean_sentiment.csv\n",
      "Empty DataFrame\n",
      "Columns: [Date, SENTIMENT_POSITIVE, SENTIMENT_NEGATIVE, SENTIMENT_UNCERTAINTY, SENTIMENT_LITIGIOUS, SENTIMENT_STRONG_MODAL, SENTIMENT_WEAK_MODAL, SENTIMENT_CONSTRAINING]\n",
      "Index: []\n",
      "\n",
      "Percentage of zeros in each column:\n",
      "Date                    0.000000\n",
      "NET_SENTIMENT          56.004532\n",
      "UNCERTAINTY_MODAL      65.445619\n",
      "REGULATORY_PRESSURE    89.463746\n",
      "dtype: float64\n",
      "\n",
      "Max values in each column:\n",
      "Date                   2021-06-29 00:00:00\n",
      "NET_SENTIMENT                          0.0\n",
      "UNCERTAINTY_MODAL                      6.0\n",
      "REGULATORY_PRESSURE                    3.0\n",
      "dtype: object\n",
      "\n",
      "Min values in each column:\n",
      "Date                   2014-01-02 00:00:00\n",
      "NET_SENTIMENT                         -7.0\n",
      "UNCERTAINTY_MODAL                      0.0\n",
      "REGULATORY_PRESSURE                    0.0\n",
      "dtype: object\n",
      "Sentiment scores saved for Sugar at ../data/news_sentiments_scores/Sugar_sentiment.csv\n",
      "Empty DataFrame\n",
      "Columns: [Date, SENTIMENT_POSITIVE, SENTIMENT_NEGATIVE, SENTIMENT_UNCERTAINTY, SENTIMENT_LITIGIOUS, SENTIMENT_STRONG_MODAL, SENTIMENT_WEAK_MODAL, SENTIMENT_CONSTRAINING]\n",
      "Index: []\n",
      "\n",
      "Percentage of zeros in each column:\n",
      "Date                    0.000000\n",
      "NET_SENTIMENT          75.850996\n",
      "UNCERTAINTY_MODAL      80.989082\n",
      "REGULATORY_PRESSURE    96.467566\n",
      "dtype: float64\n",
      "\n",
      "Max values in each column:\n",
      "Date                   2021-06-29\n",
      "NET_SENTIMENT                 0.0\n",
      "UNCERTAINTY_MODAL             6.0\n",
      "REGULATORY_PRESSURE           4.0\n",
      "dtype: object\n",
      "\n",
      "Min values in each column:\n",
      "Date                   2014-01-03\n",
      "NET_SENTIMENT                -5.0\n",
      "UNCERTAINTY_MODAL             0.0\n",
      "REGULATORY_PRESSURE           0.0\n",
      "dtype: object\n",
      "Sentiment scores saved for Wheat at ../data/news_sentiments_scores/Wheat_sentiment.csv\n"
     ]
    }
   ],
   "source": [
    "news_folder_path = '../data/news'\n",
    "news_sentiment_scores_folder_path = '../data/news_sentiments_scores'\n",
    "for filename in os.listdir(news_folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        news_file_path = os.path.join(f'{news_folder_path}/', filename)\n",
    "        ticker_name = filename.split('.')[0]\n",
    "        # Load CSV into a pandas DataFrame and parse dates\n",
    "        news_df = pd.read_csv(news_file_path, parse_dates=['published_date'], dayfirst=True)\n",
    "        # Check if the date parsing was successful\n",
    "        if news_df['published_date'].isnull().any():\n",
    "            print(f\"Warning: Some dates in '{filename}' could not be parsed.\")\n",
    "        news_sentiment_df = get_news_sentiment_score(news_df)\n",
    "        # Save the sentiment DataFrame to a CSV file\n",
    "        sentiment_scores_file_path = os.path.join(f'{news_sentiment_scores_folder_path}/', f'{ticker_name}_sentiment.csv')\n",
    "        news_sentiment_df.to_csv(sentiment_scores_file_path, index=False)  # Save without index\n",
    "        print(f'Sentiment scores saved for {ticker_name} at {sentiment_scores_file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f6130a7-9058-48e2-b1aa-3a285364766e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cocoa\n",
      "Dropped columns: ['scaled_NET_SENTIMENT', 'scaled_UNCERTAINTY_MODAL', 'scaled_REGULATORY_PRESSURE']\n",
      "Merged DataFrame saved successfully for Cocoa, with unmatched sentiment values filled with NaN.\n",
      "Coffee\n",
      "Dropped columns: ['scaled_NET_SENTIMENT', 'scaled_UNCERTAINTY_MODAL', 'scaled_REGULATORY_PRESSURE']\n",
      "Merged DataFrame saved successfully for Coffee, with unmatched sentiment values filled with NaN.\n",
      "Corn\n",
      "Dropped columns: ['scaled_NET_SENTIMENT', 'scaled_UNCERTAINTY_MODAL', 'scaled_REGULATORY_PRESSURE']\n",
      "Merged DataFrame saved successfully for Corn, with unmatched sentiment values filled with NaN.\n",
      "Cotton\n",
      "Dropped columns: ['scaled_NET_SENTIMENT', 'scaled_UNCERTAINTY_MODAL', 'scaled_REGULATORY_PRESSURE']\n",
      "Merged DataFrame saved successfully for Cotton, with unmatched sentiment values filled with NaN.\n",
      "Soya Bean\n",
      "Dropped columns: ['scaled_NET_SENTIMENT', 'scaled_UNCERTAINTY_MODAL', 'scaled_REGULATORY_PRESSURE']\n",
      "Merged DataFrame saved successfully for Soya Bean, with unmatched sentiment values filled with NaN.\n",
      "Sugar\n",
      "Dropped columns: ['scaled_NET_SENTIMENT', 'scaled_UNCERTAINTY_MODAL', 'scaled_REGULATORY_PRESSURE']\n",
      "Merged DataFrame saved successfully for Sugar, with unmatched sentiment values filled with NaN.\n",
      "Wheat\n",
      "Dropped columns: ['scaled_NET_SENTIMENT', 'scaled_UNCERTAINTY_MODAL', 'scaled_REGULATORY_PRESSURE']\n",
      "Merged DataFrame saved successfully for Wheat, with unmatched sentiment values filled with NaN.\n"
     ]
    }
   ],
   "source": [
    "output_folder_path = '../data/test/'\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Loop through each sentiment score CSV file in the folder\n",
    "for filename in os.listdir(news_sentiment_scores_folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        news_sentiment_file_path = os.path.join(news_sentiment_scores_folder_path, filename)\n",
    "        ticker_name = filename.split('_')[0]\n",
    "\n",
    "        # Load news sentiment CSV\n",
    "        news_sentiment_df = pd.read_csv(news_sentiment_file_path)\n",
    "\n",
    "        # Define output file path\n",
    "        output_file_path = os.path.join(output_folder_path, f'{ticker_name}.csv')\n",
    "\n",
    "        # Check if output file exists\n",
    "        if not os.path.isfile(output_file_path):\n",
    "            print('File does not exist in output folder')\n",
    "            break\n",
    "        else:\n",
    "            # Load output CSV\n",
    "            output_df = pd.read_csv(output_file_path, parse_dates=['Date'], dayfirst=True)\n",
    "            # Convert the 'Date' column to the desired format\n",
    "            output_df['Date'] = output_df['Date'].dt.strftime('%d/%m/%Y')\n",
    "\n",
    "        print(ticker_name)\n",
    "\n",
    "        # Define columns to drop from output_df\n",
    "        output_columns = ['scaled_NET_SENTIMENT', 'scaled_UNCERTAINTY_MODAL', 'scaled_REGULATORY_PRESSURE']\n",
    "\n",
    "        # Check for columns to drop\n",
    "        columns_to_drop = [col for col in output_columns if col in output_df.columns]\n",
    "        if columns_to_drop:\n",
    "            output_df.drop(columns=columns_to_drop, inplace=True)\n",
    "            print(f\"Dropped columns: {columns_to_drop}\")\n",
    "        else:\n",
    "            print(\"No matching columns to drop.\")\n",
    "\n",
    "        # Define unscaled sentiment columns to drop from news_sentiment_df\n",
    "        unscaled_news_sentiment_columns = ['NET_SENTIMENT', 'UNCERTAINTY_MODAL', 'REGULATORY_PRESSURE']\n",
    "        news_sentiment_df.drop(columns=unscaled_news_sentiment_columns, inplace=True)\n",
    "\n",
    "        # Step 1: Convert 'Date' to datetime format\n",
    "        news_sentiment_df['Date'] = pd.to_datetime(news_sentiment_df['Date'], dayfirst=True, errors='coerce')\n",
    "\n",
    "        # Step 2: Format the 'Date' back to the original format\n",
    "        news_sentiment_df['Date'] = news_sentiment_df['Date'].dt.strftime('%d/%m/%Y')\n",
    "\n",
    "        # Perform a left merge on the Date columns to ensure all rows from output_df are kept\n",
    "        merged_df = pd.merge(output_df, news_sentiment_df, on='Date', how='left')\n",
    "\n",
    "        # Fill missing sentiment values in the sentiment columns with NaN\n",
    "        merged_df[['scaled_NET_SENTIMENT', 'scaled_UNCERTAINTY_MODAL', 'scaled_REGULATORY_PRESSURE']] = merged_df[['scaled_NET_SENTIMENT', 'scaled_UNCERTAINTY_MODAL', 'scaled_REGULATORY_PRESSURE']].fillna(value=5)\n",
    "                \n",
    "        # Save the merged DataFrame back to the output file\n",
    "        merged_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "        print(f\"Merged DataFrame saved successfully for {ticker_name}, with unmatched sentiment values filled with NaN.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
